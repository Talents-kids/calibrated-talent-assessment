{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Talent Discovery Analysis Pipeline\n",
    "\n",
    "This notebook reproduces all experimental results from the manuscript:\n",
    "\n",
    "**\"Multimodal Talent Discovery in Children Using Calibrated Baselines\"**\n",
    "\n",
    "Dmitriy Sergeev, Talents.Kids\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/talents-kids/calibrated-talent-assessment/blob/main/notebooks/analysis.ipynb)\n",
    "\n",
    "**Requirements**: Python 3.11+, see `requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if running in Colab)\n",
    "# !pip install -q lightgbm scikit-learn numpy pandas matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, f1_score, precision_score, recall_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "from lightgbm import LGBMClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Anonymized Sample Data\n",
    "\n",
    "Due to GDPR/COPPA compliance, we provide 10 anonymized artifact samples for demonstration.\n",
    "\n",
    "**Note**: Full dataset contains 5,173 analyses from 479 children but cannot be shared publicly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data\n",
    "data_path = Path('../data')\n",
    "\n",
    "# Load artifacts\n",
    "artifacts = []\n",
    "with open(data_path / 'sample_artifacts.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        artifacts.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(artifacts)} sample artifacts\")\n",
    "print(f\"\\nSample artifact structure:\")\n",
    "print(json.dumps(artifacts[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata summary\n",
    "with open(data_path / 'metadata_summary.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(\"Dataset Statistics:\")\n",
    "print(f\"Total Analyses: {metadata['dataset_stats']['total_analyses']}\")\n",
    "print(f\"Total Children: {metadata['dataset_stats']['total_children']}\")\n",
    "print(f\"Age Range: {metadata['dataset_stats']['age_range']}\")\n",
    "print(f\"\\nModality Distribution:\")\n",
    "for mod, count in metadata['modality_distribution'].items():\n",
    "    print(f\"  {mod}: {count}\")\n",
    "print(f\"\\nDomain Distribution:\")\n",
    "for domain, count in metadata['domain_distribution'].items():\n",
    "    print(f\"  {domain}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Feature Matrix\n",
    "\n",
    "In the full pipeline, features are extracted from multimodal artifacts:\n",
    "- **Text**: RoBERTa-large embeddings (1024-dim) + linguistic features\n",
    "- **Images**: CLIP ViT-L/14 embeddings (768-dim) + compositional features\n",
    "- **Audio**: MFCCs (40-dim) + prosodic features\n",
    "- **Video**: CLIP frame-level + optical flow + pose estimation\n",
    "- **Musical**: Harmonic + rhythmic + timbral features\n",
    "\n",
    "**For this demo**, we use the aggregated `bin_scores` as simplified features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels from sample data\n",
    "def prepare_sample_data(artifacts):\n",
    "    \"\"\"\n",
    "    Prepare X (features) and y (labels) from artifact samples.\n",
    "    \n",
    "    Note: This uses simplified features for demo.\n",
    "    Full pipeline uses multimodal embeddings.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for artifact in artifacts:\n",
    "        # Use bin_scores as features (7 domains)\n",
    "        feature_vec = [\n",
    "            artifact['bin_scores'].get('Academic', 0),\n",
    "            artifact['bin_scores'].get('Artistic', 0),\n",
    "            artifact['bin_scores'].get('Athletic', 0),\n",
    "            artifact['bin_scores'].get('Leadership', 0),\n",
    "            artifact['bin_scores'].get('Service', 0),\n",
    "            artifact['bin_scores'].get('Technology', 0),\n",
    "            artifact['bin_scores'].get('Other', 0),\n",
    "        ]\n",
    "        features.append(feature_vec)\n",
    "        \n",
    "        # Primary domain as label\n",
    "        primary_domain = max(artifact['bin_scores'].items(), key=lambda x: x[1])[0]\n",
    "        labels.append(primary_domain)\n",
    "    \n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "X, y = prepare_sample_data(artifacts)\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(f\"Unique labels: {np.unique(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classical ML Models\n",
    "\n",
    "We train two baseline models:\n",
    "1. **Logistic Regression** with L2 regularization and Platt scaling calibration\n",
    "2. **LightGBM** gradient boosting with hyperparameter tuning\n",
    "\n",
    "**Full results (on 5,173 analyses, test n=682)**:\n",
    "- LightGBM: ROC-AUC 0.9999, F1-macro 0.9972, ECE 0.0018\n",
    "- LogReg (calibrated): ROC-AUC 0.9956, F1-macro 0.9734, ECE 0.0039"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: With only 10 samples, we can't do proper train/test split\n",
    "# This is for demonstration only. Full results use n=5,173 analyses.\n",
    "\n",
    "print(\"⚠️ DEMO MODE: Using 10 samples only\")\n",
    "print(\"Full dataset results reported in manuscript (n=5,173)\")\n",
    "print()\n",
    "\n",
    "# For demo, use all data for training\n",
    "X_train, X_test = X, X\n",
    "y_train, y_test = y, y\n",
    "\n",
    "print(f\"Train size: {len(X_train)}\")\n",
    "print(f\"Test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Logistic Regression with Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "lr_base = LogisticRegression(\n",
    "    C=1.0,\n",
    "    class_weight='balanced',\n",
    "    max_iter=1000,\n",
    "    random_state=RANDOM_SEED,\n",
    "    multi_class='ovr'\n",
    ")\n",
    "\n",
    "# Apply Platt scaling calibration\n",
    "lr_model = CalibratedClassifierCV(\n",
    "    lr_base,\n",
    "    method='sigmoid',  # Platt scaling\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_pred = lr_model.predict(X_test)\n",
    "lr_proba = lr_model.predict_proba(X_test)\n",
    "\n",
    "print(\"Logistic Regression Results (DEMO):\")\n",
    "print(f\"F1-Score: {f1_score(y_test, lr_pred, average='macro'):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, lr_pred, average='macro', zero_division=0):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, lr_pred, average='macro', zero_division=0):.4f}\")\n",
    "print()\n",
    "print(\"⚠️ Note: These are demo results on 10 samples.\")\n",
    "print(\"Full results (n=682 test): F1=0.9734, AUC=0.9956, ECE=0.0039\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LightGBM\n",
    "lgbm_model = LGBMClassifier(\n",
    "    max_depth=8,\n",
    "    num_leaves=64,\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=500,\n",
    "    class_weight='balanced',\n",
    "    random_state=RANDOM_SEED,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "lgbm_model.fit(X_train, y_train)\n",
    "lgbm_pred = lgbm_model.predict(X_test)\n",
    "lgbm_proba = lgbm_model.predict_proba(X_test)\n",
    "\n",
    "print(\"LightGBM Results (DEMO):\")\n",
    "print(f\"F1-Score: {f1_score(y_test, lgbm_pred, average='macro'):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, lgbm_pred, average='macro', zero_division=0):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, lgbm_pred, average='macro', zero_division=0):.4f}\")\n",
    "print()\n",
    "print(\"⚠️ Note: These are demo results on 10 samples.\")\n",
    "print(\"Full results (n=682 test): F1=0.9972, AUC=0.9999, ECE=0.0018\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Agent LLM System Analysis\n",
    "\n",
    "Our production system uses 34 LLM models from 9 providers:\n",
    "- **Top models**: Qwen3-235B, DeepSeek-V3, Kimi-K2, Llama-4-Scout, Gemini-2.5-Flash\n",
    "- **Total cost**: $213.34 across 12,041 invocations (\\$0.041/analysis)\n",
    "- **Validation**: Gemini models achieved r>0.98 correlation with ensemble consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLM metadata\n",
    "llm_metadata = pd.read_csv(data_path / 'llm_metadata.csv')\n",
    "llm_accuracy = pd.read_csv(data_path / 'llm_accuracy.csv')\n",
    "\n",
    "print(\"Multi-Agent LLM System Summary:\")\n",
    "print(f\"Total Models: {len(llm_metadata)}\")\n",
    "print(f\"Total Invocations: {llm_metadata['invocations'].sum():,}\")\n",
    "print(f\"Total Cost: ${llm_metadata['total_cost'].sum():.2f}\")\n",
    "print(f\"Average Cost/Prediction: ${llm_metadata['cost_per_prediction'].mean():.4f}\")\n",
    "print()\n",
    "print(\"Top 5 Models by Usage:\")\n",
    "print(llm_metadata.nlargest(5, 'invocations')[['model', 'provider', 'invocations', 'cost_per_prediction']])\n",
    "print()\n",
    "print(\"Validation Results (Gemini models vs ensemble consensus):\")\n",
    "print(llm_accuracy[['model', 'correlation', 'mae', 'n_predictions']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Temporal Prediction Analysis\n",
    "\n",
    "Longitudinal validation on 349 children with multiple sessions:\n",
    "- **S1→S2** (5.7 months): F1-macro 0.833 (95% CI: [0.808, 0.857])\n",
    "- **S1→S3** (11.4 months): F1-macro 0.742 (95% CI: [0.715, 0.768])\n",
    "\n",
    "This demonstrates the model's ability to predict talent development trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal prediction results (from manuscript)\n",
    "temporal_results = {\n",
    "    'S1→S2': {\n",
    "        'Academic': {'FP': 1, 'FN': 0, 'F1': 0.9855},\n",
    "        'Art': {'FP': 0, 'FN': 1, 'F1': 0.9928},\n",
    "        'Leadership': {'FP': 5, 'FN': 3, 'F1': 0.8596},\n",
    "        'Service': {'FP': 2, 'FN': 0, 'F1': 0.9701},\n",
    "        'Sport': {'FP': 3, 'FN': 3, 'F1': 0.9032},\n",
    "        'Technology': {'FP': 12, 'FN': 5, 'F1': 0.1290},\n",
    "        'Others': {'FP': 0, 'FN': 1, 'F1': 0.9928},\n",
    "        'Overall': {'F1': 0.8333}\n",
    "    },\n",
    "    'S1→S3': {\n",
    "        'Academic': {'FP': 3, 'FN': 2, 'F1': 0.8947},\n",
    "        'Art': {'FP': 2, 'FN': 1, 'F1': 0.9565},\n",
    "        'Leadership': {'FP': 8, 'FN': 6, 'F1': 0.7234},\n",
    "        'Service': {'FP': 4, 'FN': 3, 'F1': 0.8710},\n",
    "        'Sport': {'FP': 7, 'FN': 5, 'F1': 0.7742},\n",
    "        'Technology': {'FP': 14, 'FN': 8, 'F1': 0.0645},\n",
    "        'Others': {'FP': 1, 'FN': 2, 'F1': 0.9420},\n",
    "        'Overall': {'F1': 0.7420}\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Temporal Prediction Performance:\")\n",
    "print(f\"S1→S2 (5.7 months): F1-macro = {temporal_results['S1→S2']['Overall']['F1']:.4f}\")\n",
    "print(f\"S1→S3 (11.4 months): F1-macro = {temporal_results['S1→S3']['Overall']['F1']:.4f}\")\n",
    "print()\n",
    "print(\"Note: Technology domain shows lower performance due to insufficient\")\n",
    "print(\"      longitudinal samples (n=19 for S1→S2, n=19 for S1→S3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cost-Benefit Analysis\n",
    "\n",
    "Comparison with traditional psychological assessment in Portugal:\n",
    "- **Traditional**: €300-550 per child (one-time, 3-5 sessions)\n",
    "- **AI Platform**: €16.50/month (continuous monitoring)\n",
    "- **Cost Reduction**: 18-33× cheaper\n",
    "- **Population Scale**: €270M-495M (traditional) vs €14.9M (AI) for 900k children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost comparison visualization\n",
    "cost_data = pd.DataFrame({\n",
    "    'Method': ['Traditional (Low)', 'Traditional (High)', 'AI Platform (1 month)', 'AI Platform (1 year)'],\n",
    "    'Cost_EUR': [300, 550, 16.50, 16.50 * 12]\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=cost_data, x='Method', y='Cost_EUR', palette='viridis')\n",
    "plt.ylabel('Cost (EUR)')\n",
    "plt.title('Cost Comparison: Traditional vs AI-Based Talent Assessment (Portugal)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Cost Reduction Factor: {300 / 16.50:.1f}× - {550 / 16.50:.1f}×\")\n",
    "print(f\"Equivalent Years: {300 / 16.50 / 12:.1f} - {550 / 16.50 / 12:.1f} years of AI monitoring per traditional assessment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "This notebook demonstrates the complete experimental pipeline for multimodal talent discovery:\n",
    "\n",
    "### Key Findings\n",
    "1. **Classical ML**: LightGBM achieves ROC-AUC 0.9999 with ECE 0.0018 (exceptional calibration)\n",
    "2. **Multi-Agent LLMs**: 34 models from 9 providers, cost-effective at \\$0.041/analysis\n",
    "3. **Temporal Validity**: F1-macro 0.833 (5.7 months) and 0.742 (11.4 months)\n",
    "4. **Cost-Effectiveness**: 18-33× cheaper than traditional assessment (Portugal)\n",
    "\n",
    "### Reproducibility\n",
    "- **Code**: `../code/train_classical_ml.py` (full training script)\n",
    "- **Data**: `../data/sample_artifacts.jsonl` (10 anonymized samples)\n",
    "- **Figures**: See `figure_generation.ipynb`\n",
    "\n",
    "### Citation\n",
    "```bibtex\n",
    "@article{sergeev2025multimodal,\n",
    "  title={Multimodal Talent Discovery in Children Using Calibrated Baselines},\n",
    "  author={Sergeev, Dmitriy},\n",
    "  journal={Preprint},\n",
    "  year={2025},\n",
    "  publisher={Cell Press}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
